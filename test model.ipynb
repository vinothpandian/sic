{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 23:03:23.343448 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0722 23:03:23.364688 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0722 23:03:23.368232 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0722 23:03:23.402519 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating training data generator\n",
      "Found 12679 validated image filenames belonging to 5 classes.\n",
      "[INFO] Creating validation data generator\n",
      "Found 1409 validated image filenames belonging to 5 classes.\n",
      "[INFO] Creating test data generator\n",
      "Found 57 validated image filenames belonging to 5 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 68s 1us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0722 23:04:32.477871 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0722 23:04:32.478415 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0722 23:04:33.402594 140250897299264 deprecation_wrapper.py:119] From /home/vinoth/Apps/anaconda/envs/sic/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0722 23:04:34.567988 140250897299264 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train Metamorph neural network\n",
    "# https://transfer.sh/12Z5E1/weights.hdf5\n",
    "\"\"\"\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.applications.vgg19 import preprocess_input as vgg_preprocess_input\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint)\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Arguments for setting parameters while running array batch job\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "NAME = \"VGG19\"\n",
    "\n",
    "# Set verbosity\n",
    "VERBOSITY = 1\n",
    "\n",
    "# Set model configuration\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "METRICS = [\"accuracy\"]\n",
    "\n",
    "# Dataset folder information\n",
    "TRAINING_CSV = \"./training-labels.csv\"\n",
    "DATASET_FOLDER = \"./output_combined2\"\n",
    "\n",
    "\n",
    "WEIGHTS_FOLDER = \".\"\n",
    "\n",
    "\n",
    "# WEIGHTS_PATH = get_file('pretrained_weights',\n",
    "#                         'https://transfer.sh/12Z5E1/weights.hdf5')\n",
    "\n",
    "WEIGHTS_PATH = \"/home/vinoth/weights.hdf5\"\n",
    "\n",
    "# Image augmentation parameters\n",
    "\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "DEPTH = 3\n",
    "SHIFT = 20.0\n",
    "ROTATION = 10.0\n",
    "VAL_AUG_FACTOR = 0.2\n",
    "\n",
    "# Hyperparameters\n",
    "# Set epochs from args if set else from config file\n",
    "EPOCHS = 3\n",
    "\n",
    "BATCH_SIZE = 21\n",
    "LEARNING_RATE = 0.001\n",
    "DROP_EVERY = 15\n",
    "DROP_FACTOR = 0.25\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Image generator information\n",
    "TEST_SPLIT = 0.004\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "##################################################################################################\n",
    "# Read details from CSV\n",
    "###################################################################################################\n",
    "\n",
    "DATASET = pd.read_csv(TRAINING_CSV, dtype=str)\n",
    "TRAIN_VALIDATION, TEST = train_test_split(DATASET, test_size=TEST_SPLIT)\n",
    "TRAIN, VALIDATION = train_test_split(\n",
    "    TRAIN_VALIDATION, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "DATASET.head()\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "#  Create data generator to augment images for training and validation\n",
    "###################################################################################################\n",
    "\n",
    "preprocessing_function = vgg_preprocess_input\n",
    "\n",
    "TRAINING_DATA_GENERATOR = ImageDataGenerator(rotation_range=ROTATION,\n",
    "                                             width_shift_range=SHIFT,\n",
    "                                             height_shift_range=SHIFT,\n",
    "                                             preprocessing_function=preprocessing_function)\n",
    "\n",
    "VALIDATION_DATA_GENERATOR = ImageDataGenerator(rotation_range=ROTATION *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               width_shift_range=SHIFT *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               height_shift_range=SHIFT *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               preprocessing_function=preprocessing_function)\n",
    "\n",
    "TEST_DATA_GENERATOR = ImageDataGenerator(\n",
    "    preprocessing_function=preprocessing_function)\n",
    "\n",
    "COLOR_MODE = \"grayscale\" if DEPTH == 1 else \"rgb\"\n",
    "\n",
    "print(\"[INFO] Creating training data generator\")\n",
    "TRAINING_DATA = TRAINING_DATA_GENERATOR.flow_from_dataframe(dataframe=TRAIN,\n",
    "                                                            directory=DATASET_FOLDER,\n",
    "                                                            x_col=\"Filename\",\n",
    "                                                            y_col=\"Drscore\",\n",
    "                                                            class_mode=\"categorical\",\n",
    "                                                            color_mode=COLOR_MODE,\n",
    "                                                            target_size=(\n",
    "                                                                WIDTH, HEIGHT),\n",
    "                                                            batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"[INFO] Creating validation data generator\")\n",
    "VALIDATION_DATA = VALIDATION_DATA_GENERATOR.flow_from_dataframe(dataframe=VALIDATION,\n",
    "                                                                directory=DATASET_FOLDER,\n",
    "                                                                x_col=\"Filename\",\n",
    "                                                                y_col=\"Drscore\",\n",
    "                                                                class_mode=\"categorical\",\n",
    "                                                                color_mode=COLOR_MODE,\n",
    "                                                                target_size=(\n",
    "                                                                    WIDTH, HEIGHT),\n",
    "                                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"[INFO] Creating test data generator\")\n",
    "TEST_DATA = TEST_DATA_GENERATOR.flow_from_dataframe(dataframe=TEST,\n",
    "                                                    directory=DATASET_FOLDER,\n",
    "                                                    x_col=\"Filename\",\n",
    "                                                    y_col=\"Drscore\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size=(\n",
    "                                                        WIDTH, HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "NUM_OF_TRAINING_SAMPLES = 64  # len(TRAIN)\n",
    "NUM_OF_VALIDATION_SAMPLES = 64  # len(VALIDATION)\n",
    "NUM_OF_TEST_SAMPLES = len(TEST_DATA.classes)//BATCH_SIZE+1\n",
    "CLASSES = 5\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Cohen Kappa metrics\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "def cohen_kappa(y_true, y_pred):\n",
    "    y_true_classes = tf.argmax(y_true, 1)\n",
    "    y_pred_classes = tf.argmax(y_pred, 1)\n",
    "    return tf.contrib.metrics.cohen_kappa(y_true_classes, y_pred_classes, CLASSES)[1]\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# Compile MetaMorph model\n",
    "###################################################################################################\n",
    "\n",
    "BASE_MODEL = VGG19(include_top=False, input_shape=(HEIGHT, WIDTH, DEPTH))\n",
    "MODEL = Sequential()\n",
    "MODEL.add(BASE_MODEL)\n",
    "MODEL.add(GlobalAveragePooling2D())\n",
    "MODEL.add(Dense(1024, activation='relu'))\n",
    "MODEL.add(Dense(512, activation='relu'))\n",
    "MODEL.add(Dense(CLASSES, activation='softmax'))\n",
    "\n",
    "MODEL.load_weights(WEIGHTS_PATH)\n",
    "\n",
    "OPTIMISER = SGD(lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "MODEL.compile(loss=LOSS, optimizer=OPTIMISER, metrics=[*METRICS, cohen_kappa])\n",
    "\n",
    "K.get_session().run(tf.local_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_image533.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_image949.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_image720.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_image730.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_image848.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_image912.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_image17.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_image114.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_image184.tif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_image977.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id Expected\n",
       "0  test_image533.jpeg        0\n",
       "1   test_image949.jpg        0\n",
       "2  test_image720.jpeg        0\n",
       "3  test_image730.jpeg        0\n",
       "4  test_image848.jpeg        0\n",
       "5   test_image912.jpg        0\n",
       "6    test_image17.tif        0\n",
       "7   test_image114.tif        0\n",
       "8   test_image184.tif        0\n",
       "9   test_image977.jpg        0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_FILES = [ [filename, \"0\"] for filename in os.listdir(\"./Test\")[:10]]\n",
    "TEST = pd.DataFrame(TEST_FILES, columns=[\"Id\", \"Expected\"])\n",
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating test data generator\n",
      "Found 10 validated image filenames belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_GENERATOR = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "print(\"[INFO] Creating test data generator\")\n",
    "TEST_DATA = TEST_DATA_GENERATOR.flow_from_dataframe(dataframe=TEST,\n",
    "                                                    directory=\"./Test\",\n",
    "                                                    x_col=\"Id\",\n",
    "                                                    y_col=\"Expected\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size=(HEIGHT, WIDTH),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TEST_SAMPLES = len(TEST_DATA.classes)//BATCH_SIZE+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "PREDICTIONS = MODEL.predict_generator(generator=TEST_DATA,\n",
    "                                      steps=NUM_OF_TEST_SAMPLES,\n",
    "                                      verbose=VERBOSITY)\n",
    "Y_PREDICTIONS = np.argmax(PREDICTIONS, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 0, 4, 3, 3, 3, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST[\"Expected\"] = Y_PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST.to_csv(\"Submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
