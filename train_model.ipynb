{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"\n",
    "Train Metamorph neural network\n",
    "\"\"\"\n",
    "import argparse\n",
    "import configparser\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import keras.backend as K\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from imutils import paths\n",
    "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
    "                             ModelCheckpoint, TensorBoard)\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image, ImageFile\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from callbacks.trainingmonitor import TrainingMonitor\n",
    "from nn.models import Models\n",
    "from utils.plot_confusion_matrix import confusion_matrix_analysis\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "matplotlib.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"./config/default.inif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable annoying error messages\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "###################################################################################################\n",
    "# Arguments for setting parameters while running array batch job\n",
    "###################################################################################################\n",
    "\n",
    "CONFIG = configparser.ConfigParser()\n",
    "CONFIG.read(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set verbosity\n",
    "NAME = CONFIG[\"general\"][\"name\"]\n",
    "VERBOSITY = int(CONFIG[\"general\"][\"verbosity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model configuration\n",
    "MODEL_NAME = CONFIG[\"model\"][\"name\"]\n",
    "PRETRAINED_MODEL_PATH = CONFIG[\"model\"][\"pretrained_model_path\"]\n",
    "LOSS = CONFIG[\"model\"][\"loss\"]\n",
    "METRICS = CONFIG[\"model\"][\"metrics\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folder information\n",
    "DATASET_INFORMATION = CONFIG[\"dataset_information\"]\n",
    "\n",
    "TRAINING_CSV = DATASET_INFORMATION[\"training_csv\"]\n",
    "DATASET_FOLDER = DATASET_INFORMATION[\"dataset_folder\"]\n",
    "\n",
    "# Disabled sample temporarily -\n",
    "# add later for enhancements in featurewise center\n",
    "# SAMPLE_FOLDER = DATASET_INFORMATION[\"samples folder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_TIMESTAMP = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "PID = os.getpid()\n",
    "\n",
    "OUTPUT_FOLDERNAME = f'{NAME}__{CURRENT_TIMESTAMP}__{PID}'\n",
    "\n",
    "OUTPUT_FOLDER = os.path.join(\n",
    "    DATASET_INFORMATION[\"output_folder\"], OUTPUT_FOLDERNAME)\n",
    "WEIGHTS_FOLDER = os.path.join(OUTPUT_FOLDER, \"weights\")\n",
    "LOGS_FOLDER = os.path.join(OUTPUT_FOLDER, \"logs\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "os.makedirs(WEIGHTS_FOLDER, exist_ok=True)\n",
    "os.makedirs(LOGS_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image augmentation parameters\n",
    "IMAGE_AUGMENTATION = CONFIG[\"image_augmentation\"]\n",
    "\n",
    "HEIGHT = int(IMAGE_AUGMENTATION[\"height\"])\n",
    "WIDTH = int(IMAGE_AUGMENTATION[\"width\"])\n",
    "DEPTH = int(IMAGE_AUGMENTATION[\"depth\"])\n",
    "SHIFT = float(IMAGE_AUGMENTATION[\"shift\"])\n",
    "ROTATION = float(IMAGE_AUGMENTATION[\"rotation\"])\n",
    "VAL_AUG_FACTOR = float(\n",
    "    IMAGE_AUGMENTATION[\"validation_data_augmentation_factor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HYPERPARAMETERS = CONFIG[\"hyperparameters\"]\n",
    "\n",
    "# Set epochs from args if set else from config file\n",
    "EPOCHS = ARGS[\"epochs\"] if ARGS[\"epochs\"] else int(HYPERPARAMETERS[\"epochs\"])\n",
    "\n",
    "BATCH_SIZE = int(HYPERPARAMETERS[\"batch_size\"])\n",
    "LEARNING_RATE = float(HYPERPARAMETERS[\"learning_rate\"])\n",
    "DROP_EVERY = float(HYPERPARAMETERS[\"learning_rate_decay_after_x_epoch\"])\n",
    "DROP_FACTOR = float(HYPERPARAMETERS[\"decay_rate\"])\n",
    "MOMENTUM = float(HYPERPARAMETERS[\"momentum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generator information\n",
    "TRAIN_TEST_VAL_SPLIT = CONFIG[\"train_test_val_split\"]\n",
    "\n",
    "TEST_SPLIT = float(TRAIN_TEST_VAL_SPLIT[\"test_split\"])\n",
    "VALIDATION_SPLIT = float(TRAIN_TEST_VAL_SPLIT[\"validation_split\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# print details of the model for log\n",
    "###################################################################################################\n",
    "\n",
    "print(80*\"#\")\n",
    "print(f'Training - with {NAME} config file'.center(80))\n",
    "print(CURRENT_TIMESTAMP.center(80))\n",
    "print(f'Training results stored in {OUTPUT_FOLDERNAME}'.center(80))\n",
    "print(80*\"#\")\n",
    "\n",
    "PRETTY = PrettyTable()\n",
    "PRETTY.field_names = [\"Purpose\", \"Parameter\", \"Values\"]\n",
    "PRETTY.align = \"l\"\n",
    "PRETTY.add_row([\"\", \"\", \"\"])\n",
    "PRETTY.add_row([\"Image Augmentation\", \"Shift\", SHIFT])\n",
    "PRETTY.add_row([\"Image Augmentation\", \"Rotation\", ROTATION])\n",
    "PRETTY.add_row(\n",
    "    [\"Image Augmentation\", \"Factor in validation set\", VAL_AUG_FACTOR])\n",
    "PRETTY.add_row([\"\", \"\", \"\"])\n",
    "PRETTY.add_row([\"Training\", \"Epochs\", EPOCHS])\n",
    "PRETTY.add_row([\"Training\", \"Batch size\", BATCH_SIZE])\n",
    "PRETTY.add_row([\"\", \"\", \"\"])\n",
    "PRETTY.add_row([\"Learning rate\", \"Initial learning rate\", LEARNING_RATE])\n",
    "PRETTY.add_row([\"Learning rate\", \"Drop after every \", DROP_EVERY])\n",
    "PRETTY.add_row([\"Learning rate\", \"Drop factor \", DROP_FACTOR])\n",
    "PRETTY.add_row([\"Learning rate\", \"Momentum\", MOMENTUM])\n",
    "\n",
    "DETAILS = PRETTY.get_string(title='ConvNet details')\n",
    "print(DETAILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Read details from CSV\n",
    "###################################################################################################\n",
    "\n",
    "DATASET = pd.read_csv(TRAINING_CSV, dtype=str)\n",
    "TRAIN_VALIDATION, TEST = train_test_split(DATASET, test_size=TEST_SPLIT)\n",
    "TRAIN, VALIDATION = train_test_split(\n",
    "    TRAIN_VALIDATION, test_size=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_TRAINING_SAMPLES = len(TRAIN)\n",
    "NUM_OF_VALIDATION_SAMPLES = len(VALIDATION)\n",
    "NUM_OF_TEST_SAMPLES = len(TEST)\n",
    "\n",
    "CLASSES = len(DATASET[\"Drscore\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#  Create data generator to augment images for training and validation\n",
    "###################################################################################################\n",
    "\n",
    "TRAINING_DATA_GENERATOR = ImageDataGenerator(rotation_range=ROTATION,\n",
    "                                             width_shift_range=SHIFT, height_shift_range=SHIFT,\n",
    "                                             rescale=1./255)\n",
    "\n",
    "\n",
    "VALIDATION_DATA_GENERATOR = ImageDataGenerator(rotation_range=ROTATION *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               width_shift_range=SHIFT *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               height_shift_range=SHIFT *\n",
    "                                               (1+VAL_AUG_FACTOR),\n",
    "                                               rescale=1./255)\n",
    "\n",
    "TEST_DATA_GENERATOR = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample images to fit image generator for identifying featurewise center\n",
    "# SAMPLES = [process_sample(image_path, depth=DEPTH)\n",
    "#            for image_path in paths.list_images(SAMPLE_FOLDER)]\n",
    "# TRAINING_DATA_GENERATOR.fit(SAMPLES)\n",
    "# VALIDATION_DATA_GENERATOR.fit(SAMPLES)\n",
    "\n",
    "COLOR_MODE = \"grayscale\" if DEPTH == 1 else \"rgb\"\n",
    "\n",
    "print(\"[INFO] Creating training data generator\")\n",
    "TRAINING_DATA = TRAINING_DATA_GENERATOR.flow_from_dataframe(dataframe=TRAIN,\n",
    "                                                            directory=DATASET_FOLDER,\n",
    "                                                            x_col=\"Filename\",\n",
    "                                                            y_col=\"Drscore\",\n",
    "                                                            class_mode=\"categorical\",\n",
    "                                                            color_mode=COLOR_MODE,\n",
    "                                                            target_size=(\n",
    "                                                                WIDTH, HEIGHT),\n",
    "                                                            batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"[INFO] Creating validation data generator\")\n",
    "VALIDATION_DATA = VALIDATION_DATA_GENERATOR.flow_from_dataframe(dataframe=VALIDATION,\n",
    "                                                                directory=DATASET_FOLDER,\n",
    "                                                                x_col=\"Filename\",\n",
    "                                                                y_col=\"Drscore\",\n",
    "                                                                class_mode=\"categorical\",\n",
    "                                                                color_mode=COLOR_MODE,\n",
    "                                                                target_size=(\n",
    "                                                                    WIDTH, HEIGHT),\n",
    "                                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"[INFO] Creating test data generator\")\n",
    "TEST_DATA = TEST_DATA_GENERATOR.flow_from_dataframe(dataframe=TEST,\n",
    "                                                    directory=DATASET_FOLDER,\n",
    "                                                    x_col=\"Filename\",\n",
    "                                                    y_col=\"Drscore\",\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    target_size=(\n",
    "                                                        WIDTH, HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohen Kappa metrics\n",
    "def cohen_kappa(y_true, y_pred):\n",
    "    y_true_classes = tf.argmax(y_true, 1)\n",
    "    y_pred_classes = tf.argmax(y_pred, 1)\n",
    "    return tf.contrib.metrics.cohen_kappa(y_true_classes, y_pred_classes, CLASSES)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Compile MetaMorph model\n",
    "###################################################################################################\n",
    "print(\"[INFO] Compiling model....\")\n",
    "Models = Models(height=HEIGHT, width=WIDTH, depth=DEPTH, classes=CLASSES)\n",
    "\n",
    "if PRETRAINED_MODEL_PATH != \"None\":\n",
    "    print(\"[INFO] Loading pre-trained model\")\n",
    "    MODEL = load_model(PRETRAINED_MODEL_PATH)\n",
    "else:\n",
    "    if MODEL_NAME == \"resnet\":\n",
    "        MODEL = Models.resnet50()\n",
    "    elif MODEL_NAME == \"inception\":\n",
    "        MODEL = Models.inception()\n",
    "    else:\n",
    "        MODEL = Models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMISER = SGD(lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "MODEL.compile(loss=LOSS, optimizer=OPTIMISER, metrics=[*METRICS, cohen_kappa])\n",
    "\n",
    "K.get_session().run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(80*\"#\")\n",
    "print(\"\\n\")\n",
    "print(MODEL.name.center(80))\n",
    "print(\"\\n\")\n",
    "print(80*\"#\")\n",
    "MODEL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Define callbacks\n",
    "###################################################################################################\n",
    "\n",
    "# Training monitor\n",
    "FIGURE_PATH = os.path.sep.join([LOGS_FOLDER, \"{}.png\".format(PID)])\n",
    "JSON_PATH = os.path.sep.join([LOGS_FOLDER, \"{}.json\".format(PID)])\n",
    "TRAINING_MONITOR = TrainingMonitor(FIGURE_PATH, jsonPath=JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    \"\"\"Generate step decay callback function\n",
    "\n",
    "    Arguments:\n",
    "        epoch {int} -- Current epoch value\n",
    "\n",
    "    Returns:\n",
    "        float -- Updated learning rate alpha after decay\n",
    "    \"\"\"\n",
    "    alpha = LEARNING_RATE * (DROP_FACTOR ** np.floor((1 + epoch) / DROP_EVERY))\n",
    "    return float(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate decay in steps\n",
    "DECAY = LearningRateScheduler(step_decay)\n",
    "\n",
    "# Checkpoint model callback\n",
    "WEIGHT_NAME = os.path.join(WEIGHTS_FOLDER, \"weights.hdf5\")\n",
    "CHECKPOINT = ModelCheckpoint(WEIGHT_NAME, monitor=\"val_loss\", mode=\"min\",\n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "EARLY_STOP = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0.001,\n",
    "                           patience=5,\n",
    "                           mode='min',\n",
    "                           verbose=1)\n",
    "\n",
    "TENSORBOARD = TensorBoard(log_dir=LOGS_FOLDER,\n",
    "                          histogram_freq=0,\n",
    "                          # write_batch_performance=True,\n",
    "                          write_graph=True,\n",
    "                          write_images=False)\n",
    "\n",
    "CALLBACKS = [EARLY_STOP, TRAINING_MONITOR, DECAY, CHECKPOINT, TENSORBOARD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Train the model\n",
    "###################################################################################################\n",
    "\n",
    "print(\"[INFO] Training the model....\")\n",
    "HISTORY = MODEL.fit_generator(generator=TRAINING_DATA,\n",
    "                              steps_per_epoch=NUM_OF_TRAINING_SAMPLES//BATCH_SIZE,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=CALLBACKS,\n",
    "                              validation_data=VALIDATION_DATA,\n",
    "                              validation_steps=NUM_OF_VALIDATION_SAMPLES//BATCH_SIZE,\n",
    "                              verbose=VERBOSITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Storing the model to output\n",
    "###################################################################################################\n",
    "\n",
    "print(\"[INFO] Storing trained model....\")\n",
    "MODEL.save(os.path.join(OUTPUT_FOLDER, \"trained_model.hdf5\"))\n",
    "MODEL.save_weights(os.path.join(OUTPUT_FOLDER, \"trained_weights.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "# Evaluate the model and store the report and history log\n",
    "###################################################################################################\n",
    "\n",
    "print(\"[INFO] Evaluating the model....\")\n",
    "# Predict only on existing images - len(TEST_DATA.classes)\n",
    "PREDICTIONS = MODEL.predict_generator(generator=TEST_DATA,\n",
    "                                      steps=NUM_OF_TEST_SAMPLES//BATCH_SIZE)\n",
    "Y_PREDICTIONS = np.argmax(PREDICTIONS, axis=1)\n",
    "\n",
    "CONFUSION_MATRIX_FILENAME = os.path.join(OUTPUT_FOLDER, \"confusion_matrix\")\n",
    "\n",
    "confusion_matrix_analysis(y_true=TEST_DATA.classes,\n",
    "                          y_predicted=Y_PREDICTIONS,\n",
    "                          filename=CONFUSION_MATRIX_FILENAME,\n",
    "                          labels=np.arange(CLASSES))\n",
    "\n",
    "CLASSIFICATION_REPORT = classification_report(TEST_DATA.classes,\n",
    "                                              Y_PREDICTIONS)\n",
    "print(CLASSIFICATION_REPORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY = HISTORY.history[\"acc\"][-1] * 100\n",
    "VALIDATION_ACCURACY = HISTORY.history[\"val_acc\"][-1] * 100\n",
    "LOSS = HISTORY.history[\"loss\"][-1]\n",
    "VALIDATION_LOSS = HISTORY.history[\"val_loss\"][-1]\n",
    "\n",
    "REPORT = [\n",
    "    80*\"#\",\n",
    "    \"\\n\",\n",
    "    \"REPORT\".center(80),\n",
    "    f'Training with {NAME} config'.center(80),\n",
    "    f'Config file : {CONFIG_FILE}'.center(80),\n",
    "    f'Model name: {MODEL.name}'.center(80),\n",
    "    f'Time: {CURRENT_TIMESTAMP}'.center(80),\n",
    "    f'Training results stored in {OUTPUT_FOLDERNAME}'.center(80),\n",
    "    \"\\n\",\n",
    "    80*\"#\",\n",
    "    \"\\n\",\n",
    "    DETAILS,\n",
    "    \"\\n\",\n",
    "    f'Accuracy: {ACCURACY:.2f}',\n",
    "    f'Validation accuracy: {VALIDATION_ACCURACY:.2f}',\n",
    "    f'Loss: {LOSS:.4f}',\n",
    "    f'Validation Loss: {VALIDATION_LOSS:.4f}',\n",
    "    \"\\n\",\n",
    "    80*\"#\",\n",
    "    \"\\n\",\n",
    "    \"Evaluation Metrics\",\n",
    "    \"\\n\",\n",
    "    \"Classification report\",\n",
    "    CLASSIFICATION_REPORT,\n",
    "    \"\\n\",\n",
    "    80*\"#\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0e89fd9cb351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training - with {NAME} config file'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCURRENT_TIMESTAMP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training results stored in {OUTPUT_FOLDERNAME}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NAME' is not defined"
     ]
    }
   ],
   "source": [
    "for line in REPORT:\n",
    "    print(line)\n",
    "\n",
    "REPORT_SHORTFORM = f'{ACCURACY:.0f}_{VALIDATION_ACCURACY:.0f}_{LOSS:.2f}_{VALIDATION_LOSS:.2f}'\n",
    "\n",
    "FILENAME = f'REPORT__{REPORT_SHORTFORM}.txt'\n",
    "\n",
    "print(\"[INFO] Storing the evaluation results....\")\n",
    "with open(os.path.join(OUTPUT_FOLDER, FILENAME), \"w\") as eval_result:\n",
    "    eval_result.write(\"\\n\".join(REPORT))\n",
    "\n",
    "shutil.copy(CONFIG_FILE, OUTPUT_FOLDER)\n",
    "os.rename(OUTPUT_FOLDER, f'{OUTPUT_FOLDER}__{REPORT_SHORTFORM}')\n",
    "print(\"[INFO] Training complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
